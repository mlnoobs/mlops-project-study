{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "12256fb1",
      "metadata": {
        "id": "12256fb1"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mlnoobs/mlops-project-study/blob/main/2022-05-30/training_remind_seminar.ipynb)\n",
        "\n",
        "# 딥러닝 remind 세미나 1\n",
        "taylor (2022-05-30)\n",
        "\n",
        "- Linear(Fully-Connected layer), CNN(Convolutional neural network) 훈련 및 인퍼런스를 PyTorch를 사용해 google colab GPU에서 시도합니다.\n",
        "- 노트북 제작에 있어 다음 레퍼런스를 많이 참고했습니다.\n",
        "  - https://github.com/espnet/notebook/blob/master/tts_realtime_demo.ipynb\n",
        "  - https://github.com/pytorch/examples/blob/main/mnist/main.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 필요 툴&라이브러리 설치"
      ],
      "metadata": {
        "id": "_ck6yHmNfb7F"
      },
      "id": "_ck6yHmNfb7F"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n"
      ],
      "metadata": {
        "id": "kPHKc8y7fqMk"
      },
      "id": "kPHKc8y7fqMk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import & 버전 확인"
      ],
      "metadata": {
        "id": "N_XAZJXsgyY5"
      },
      "id": "N_XAZJXsgyY5"
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version\n",
        "!nvidia-smi\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "print(torch.__version__)\n",
        "print(np.__version__)\n",
        "print(torchvision.__version__)\n"
      ],
      "metadata": {
        "id": "jEIP5rscg3ZD"
      },
      "id": "jEIP5rscg3ZD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperpatemerers 등 configs 세팅"
      ],
      "metadata": {
        "id": "4t53bdHUjmDC"
      },
      "id": "4t53bdHUjmDC"
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 결과 재현을 위한 seed값 고정\n",
        "seed = '202205281708'\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "\n",
        "# Hyperparameters\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "lr = 1e-2\n",
        "n_epochs = 10\n",
        "batch_size = 512\n",
        "\n",
        "print(\"device:\", device)\n",
        "print(\"- gpu를 사용할 수 있는 환경이면 \\'cuda\\', 그렇지 않으면 \\'cpu\\'를 사용합니다.\")\n",
        "print(\"lr:\", lr)\n",
        "print(\"- learning rate는 이번 훈련으로 얻어낸 gradients를 얼마만큼의 강도로 모델 업데이트에 반영할지를 결정합니다.\")\n",
        "print(\"n_epochs:\", n_epochs)\n",
        "print(\"- epoch의 횟수는 전체 데이터셋을 몇 번 반복해서 훈련할 것인지를 결정합니다.\")\n",
        "print(\"batch_size:\", batch_size)\n",
        "print(\"- 훈련 시 mini-batch의 사이즈를 결정합니다.\")\n",
        "print(\"- SGD(stochastic gradient descent) 기반의 optimizer를 사용한다면,\")\n",
        "print(\"- 전체 데이터셋을 자그마한 mini-batch로 나누어서 순차적으로 훈련에 사용하게 됩니다.\")\n",
        "print(\"- 보통 클수록 보다 성능을 높이는 방향으로 훈련될 가능성이 높지만 메모리를 많이 차지하게 됩니다.\")"
      ],
      "metadata": {
        "id": "bduUNYwgjtE0"
      },
      "id": "bduUNYwgjtE0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 공개 데이터셋 다운로드: MNIST\n",
        "torchvision 라이브러리를 사용하면 공개 데이터셋을 편리하게 다운로드받고, torch tensor 자료형으로 불러올 수 있습니다."
      ],
      "metadata": {
        "id": "N6HFmHqphvVO"
      },
      "id": "N6HFmHqphvVO"
    },
    {
      "cell_type": "code",
      "source": [
        "# torchvision 라이브러리를 사용해 MNIST 데이터셋 다운로드 및 전처리\n",
        "transform = torchvision.transforms.Compose([\n",
        "                torchvision.transforms.ToTensor(),\n",
        "                torchvision.transforms.Normalize((0.1307,), (0.3081,)) # MNIST 데이터셋의 평균과 표준편차\n",
        "            ])\n",
        "train_data = torchvision.datasets.MNIST('./mnist_dataset', train=True, download=True,\n",
        "                            transform=transform)\n",
        "test_data = torchvision.datasets.MNIST('./mnist_dataset', train=False, download=True,\n",
        "                            transform=transform)\n",
        "print('len(train_data):', len(train_data))\n",
        "print('len(test_data):', len(test_data))\n",
        "\n",
        "# 데이터 정보를 확인해봅시다.\n",
        "sample = train_data[1234]\n",
        "print('shape:', sample[0].shape)\n",
        "print('label:', sample[1])\n",
        "print('maxval:', torch.max(sample[0]))\n",
        "print('minval:', torch.min(sample[0]))\n",
        "train_data_without_labels = torch.stack([s[0] for s in train_data])\n",
        "print('mean of all training dataset:', torch.mean(train_data_without_labels))\n",
        "print('var of all training dataset:', torch.var(train_data_without_labels))\n",
        "plt.imshow(sample[0][0])\n",
        "plt.show()\n",
        "\n",
        "# torch의 DataLoader를 사용하면 데이터셋을 준비하는 과정이 편리합니다.\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
        "                                          batch_size=1000,\n",
        "                                          shuffle=True)"
      ],
      "metadata": {
        "id": "zmt6BsoYiiQq"
      },
      "id": "zmt6BsoYiiQq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 훈련 & 테스트 코드"
      ],
      "metadata": {
        "id": "N4ffWtEayY-t"
      },
      "id": "N4ffWtEayY-t"
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, device, train_loader, optimizer, n_epochs, fn_loss,\n",
        "                log_interval=10000):\n",
        "    model.train()\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = fn_loss(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if batch_idx % (log_interval // len(data)) == 0:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                      epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                      100. * batch_idx / len(train_loader), loss.item()))\n",
        "            \n",
        "def test_model(model, device, test_loader, fn_loss):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "id": "36NR-n81yb0l"
      },
      "id": "36NR-n81yb0l",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예제 1: Linear layer 1개"
      ],
      "metadata": {
        "id": "NFfzHcVFq0dN"
      },
      "id": "NFfzHcVFq0dN"
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelV1(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ModelV1, self).__init__()\n",
        "        self.input_size = 28*28\n",
        "        self.output_size = 10\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            torch.nn.Linear(self.input_size, self.output_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.input_size)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "print(\"loading ModelV1...\") \n",
        "model = ModelV1().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "fn_loss = F.nll_loss\n",
        "print(\"loaded ModelV1 to {}\".format(device))\n",
        "print(model)\n",
        "print('Number of model parameters: ', sum(param.numel() for param in model.parameters()))\n",
        "print(\"device:\", device)\n",
        "print(\"lr:\", lr)\n",
        "print(\"n_epochs:\", n_epochs)\n",
        "print(\"batch_size:\", batch_size)\n",
        "\n",
        "t = time.perf_counter()\n",
        "train_model(model, device, train_loader, optimizer, n_epochs, fn_loss, log_interval=60000)\n",
        "test_model(model, device, test_loader, fn_loss)\n",
        "print(\"Train/Eval done. elapsed: {} sec\".format(time.perf_counter() - t))\n",
        "\n",
        "del model\n",
        "del optimizer\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "7jeFas2enrEe"
      },
      "id": "7jeFas2enrEe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예제 2: Linear layer 3개"
      ],
      "metadata": {
        "id": "uezf4X4a5oHD"
      },
      "id": "uezf4X4a5oHD"
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "del optimizer\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "class ModelV2(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ModelV2, self).__init__()\n",
        "        self.input_size = 28*28\n",
        "        self.hidden_size = 256\n",
        "        self.output_size = 10\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            torch.nn.Linear(self.input_size, self.hidden_size),\n",
        "            torch.nn.Linear(self.hidden_size, self.hidden_size),\n",
        "            torch.nn.Linear(self.hidden_size, self.output_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.input_size)\n",
        "        for idx, layer in enumerate(self.layers):\n",
        "            x = layer(x)\n",
        "            x = F.relu(x) if idx != len(self.layers)-1 else x\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "print(\"loading ModelV2...\") \n",
        "model = ModelV2().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "fn_loss = F.nll_loss\n",
        "print(\"loaded ModelV2 to {}\".format(device))\n",
        "print(model)\n",
        "print('Number of model parameters: ', sum(param.numel() for param in model.parameters()))\n",
        "print(\"device:\", device)\n",
        "print(\"lr:\", lr)\n",
        "print(\"n_epochs:\", n_epochs)\n",
        "print(\"batch_size:\", batch_size)\n",
        "\n",
        "t = time.perf_counter()\n",
        "train_model(model, device, train_loader, optimizer, n_epochs, fn_loss, log_interval=60000)\n",
        "test_model(model, device, test_loader, fn_loss)\n",
        "print(\"Train/Eval done. elapsed: {} sec\".format(time.perf_counter() - t))\n",
        "\n",
        "del model\n",
        "del optimizer\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "hbXnlqCr5uYm"
      },
      "id": "hbXnlqCr5uYm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예제 3: CNN, linear, 각종 함수 마음껏"
      ],
      "metadata": {
        "id": "Vu9CjxFr-Fqf"
      },
      "id": "Vu9CjxFr-Fqf"
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "del optimizer\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "class ModelV3(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ModelV3, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = torch.nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = torch.nn.Dropout(0.25)\n",
        "        self.dropout2 = torch.nn.Dropout(0.5)\n",
        "        self.fc1 = torch.nn.Linear(9216, 128)\n",
        "        self.fc2 = torch.nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "print(\"loading ModelV3...\") \n",
        "model = ModelV3().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "fn_loss = F.nll_loss\n",
        "print(\"loaded ModelV3 to {}\".format(device))\n",
        "print(model)\n",
        "print('Number of model parameters: ', sum(param.numel() for param in model.parameters()))\n",
        "print(\"device:\", device)\n",
        "print(\"lr:\", lr)\n",
        "print(\"n_epochs:\", n_epochs)\n",
        "print(\"batch_size:\", batch_size)\n",
        "\n",
        "t = time.perf_counter()\n",
        "train_model(model, device, train_loader, optimizer, n_epochs, fn_loss, log_interval=60000)\n",
        "test_model(model, device, test_loader, fn_loss)\n",
        "print(\"Train/Eval done. elapsed: {} sec\".format(time.perf_counter() - t))\n",
        "\n",
        "del model\n",
        "del optimizer\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "qT8Vx9Wq-etV"
      },
      "id": "qT8Vx9Wq-etV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예제 4: ModelV3의 optimizer를 Adam으로 교체, LR decaying 적용"
      ],
      "metadata": {
        "id": "YcfmyubEH1CO"
      },
      "id": "YcfmyubEH1CO"
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "del optimizer\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "def train_model_scheduler(model, device, train_loader, optimizer, n_epochs, fn_loss,\n",
        "                          scheduler, log_interval=10000):\n",
        "    model.train()\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = fn_loss(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if batch_idx % (log_interval // len(data)) == 0:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                      epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                      100. * batch_idx / len(train_loader), loss.item()))\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "print(\"loading ModelV3...\") \n",
        "model = ModelV3().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "fn_loss = F.nll_loss\n",
        "\n",
        "print(\"loaded ModelV3 to {}\".format(device))\n",
        "print(model)\n",
        "print('Number of model parameters: ', sum(param.numel() for param in model.parameters()))\n",
        "print(\"device:\", device)\n",
        "print(\"lr:\", lr)\n",
        "print(\"n_epochs:\", n_epochs)\n",
        "print(\"batch_size:\", batch_size)\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
        "\n",
        "t = time.perf_counter()\n",
        "train_model_scheduler(model, device, train_loader, optimizer, n_epochs, fn_loss, scheduler, log_interval=60000)\n",
        "test_model(model, device, test_loader, fn_loss)\n",
        "print(\"Train/Eval done. elapsed: {} sec\".format(time.perf_counter() - t))\n",
        "\n",
        "del model\n",
        "del optimizer\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "_wOObXD0I9ol"
      },
      "id": "_wOObXD0I9ol",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예제 5: 훈련 완료된 빅 모델 가져와서 전이 학습(transfer learning)\n",
        "원래 ImageNet이라는 훨씬 큰 classification dataset에 맞춰서 훈련이 완료된 모델을 가져와서 MNIST 맞춤형으로 전이 학습을 시도\n",
        "\n"
      ],
      "metadata": {
        "id": "biViwCiVs87B"
      },
      "id": "biViwCiVs87B"
    },
    {
      "cell_type": "code",
      "source": [
        "# ref: https://ingu627.github.io/code/ResNet50_pytorch/\n",
        "class MNISTBigModel(torch.nn.Module): # MnistResNet은 nn.Module 상속\n",
        "    def __init__(self, in_channels=1):\n",
        "        super(MNISTBigModel, self).__init__()\n",
        "\n",
        "        # torchvision.models에서 사전훈련된 resnet 모델 가져오기\n",
        "        self.model = torchvision.models.resnet50(pretrained=True)\n",
        "\n",
        "        # 기본 채널이 3(RGB)이기 때문에 fashion_mnist에 맞게 1(grayscale image)로 바꿔준다.  \n",
        "        # 원래 ResNet의 첫번째 층\n",
        "        # self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.model.conv1 = torch.nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "    \n",
        "        # 1000개 클래스 대신 10개 클래스로 바꿔주기\n",
        "        num_ftrs = self.model.fc.in_features\n",
        "        #  nn.Linear(in_features, out_features ...)\n",
        "        self.model.fc = torch.nn.Linear(num_ftrs, 10)\n",
        "        #num_ftrs = self.model.fc.out_features\n",
        "        #self.projection_layer = torch.nn.Linear(num_ftrs, 10)\n",
        "\n",
        "    def forward(self, x): # 모델에 있는 foward 함수 그대로 가져오기\n",
        "        #with torch.no_grad():\n",
        "        x = self.model(x)\n",
        "        #x = self.projection_layer(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "lr = 3e-6\n",
        "n_epochs = 3\n",
        "batch_size = 64\n",
        "\n",
        "# MNIST 그림의 크기를 ImageNet 데이터와 맞추기 위해 데이터셋을 다시 로드\n",
        "transform = torchvision.transforms.Compose([\n",
        "                torchvision.transforms.Resize((224, 224)),\n",
        "                torchvision.transforms.ToTensor(),\n",
        "                torchvision.transforms.Normalize((0.1307,), (0.3081,)) # MNIST 데이터셋의 평균과 표준편차\n",
        "            ])\n",
        "train_data = torchvision.datasets.MNIST('./mnist_dataset', train=True, download=True,\n",
        "                            transform=transform)\n",
        "test_data = torchvision.datasets.MNIST('./mnist_dataset', train=False, download=True,\n",
        "                            transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
        "                                          batch_size=10,\n",
        "                                          shuffle=True)\n",
        "\n",
        "print(\"loading MNISTBigModel...\") \n",
        "model = MNISTBigModel().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "fn_loss = F.nll_loss\n",
        "\n",
        "print(\"loaded MNISTBigModel to {}\".format(device))\n",
        "print(model)\n",
        "print('Number of model parameters: ', sum(param.numel() for param in model.parameters()))\n",
        "print(\"device:\", device)\n",
        "print(\"lr:\", lr)\n",
        "print(\"n_epochs:\", n_epochs)\n",
        "print(\"batch_size:\", batch_size)\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
        "\n",
        "t = time.perf_counter()\n",
        "train_model_scheduler(model, device, train_loader, optimizer, n_epochs, fn_loss, scheduler, log_interval=10000)\n",
        "test_model(model, device, test_loader, fn_loss)\n",
        "print(\"Train/Eval done. elapsed: {} sec\".format(time.perf_counter() - t))\n",
        "\n",
        "del model\n",
        "del optimizer\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auNo8-GCtfq9",
        "outputId": "03ca19dc-24d2-4668-a7c0-d54cf2873065"
      },
      "id": "auNo8-GCtfq9",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading MNISTBigModel...\n",
            "loaded MNISTBigModel to cuda\n",
            "MNISTBigModel(\n",
            "  (model): ResNet(\n",
            "    (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (4): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (5): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "Number of model parameters:  23522250\n",
            "device: cuda\n",
            "lr: 3e-06\n",
            "n_epochs: 3\n",
            "batch_size: 64\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.468372\n",
            "Train Epoch: 1 [9984/60000 (17%)]\tLoss: 1.174733\n",
            "Train Epoch: 1 [19968/60000 (33%)]\tLoss: 0.392050\n",
            "Train Epoch: 1 [29952/60000 (50%)]\tLoss: 0.228549\n",
            "Train Epoch: 1 [39936/60000 (67%)]\tLoss: 0.134468\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.095007\n",
            "Train Epoch: 1 [59904/60000 (100%)]\tLoss: 0.112732\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.083218\n",
            "Train Epoch: 2 [9984/60000 (17%)]\tLoss: 0.064563\n",
            "Train Epoch: 2 [19968/60000 (33%)]\tLoss: 0.072744\n",
            "Train Epoch: 2 [29952/60000 (50%)]\tLoss: 0.051085\n",
            "Train Epoch: 2 [39936/60000 (67%)]\tLoss: 0.021912\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.021106\n",
            "Train Epoch: 2 [59904/60000 (100%)]\tLoss: 0.038463\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.023147\n",
            "Train Epoch: 3 [9984/60000 (17%)]\tLoss: 0.051638\n",
            "Train Epoch: 3 [19968/60000 (33%)]\tLoss: 0.030683\n",
            "Train Epoch: 3 [29952/60000 (50%)]\tLoss: 0.020306\n",
            "Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.079340\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.060572\n",
            "Train Epoch: 3 [59904/60000 (100%)]\tLoss: 0.006040\n",
            "\n",
            "Test set: Average loss: 0.0283, Accuracy: 9926/10000 (99%)\n",
            "\n",
            "Train/Eval done. elapsed: 2128.349693376 sec\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "name": "training_remind_seminar.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}