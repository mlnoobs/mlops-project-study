{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12256fb1",
   "metadata": {
    "id": "12256fb1"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mlnoobs/mlops-project-study/blob/main/2022-05-30/training_remind_seminar.ipynb)\n",
    "\n",
    "# 딥러닝 remind 세미나 1\n",
    "taylor (2022-05-30)\n",
    "\n",
    "- Linear(Fully-Connected layer), CNN(Convolutional neural network) 훈련 및 인퍼런스를 PyTorch를 사용해 google colab GPU에서 시도합니다.\n",
    "- 노트북 제작에 있어 다음 레퍼런스를 많이 참고했습니다.\n",
    "  - https://github.com/espnet/notebook/blob/master/tts_realtime_demo.ipynb\n",
    "  - https://github.com/pytorch/examples/blob/main/mnist/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_ck6yHmNfb7F",
   "metadata": {
    "id": "_ck6yHmNfb7F"
   },
   "source": [
    "## 필요 툴&라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "kPHKc8y7fqMk",
   "metadata": {
    "id": "kPHKc8y7fqMk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/ruby/opt/anaconda3/lib/python3.8/site-packages (22.1.2)\n",
      "Requirement already satisfied: torch in /Users/ruby/opt/anaconda3/lib/python3.8/site-packages (1.11.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/ruby/opt/anaconda3/lib/python3.8/site-packages (from torch) (4.0.1)\n",
      "Requirement already satisfied: torchvision in /Users/ruby/opt/anaconda3/lib/python3.8/site-packages (0.12.0)\n",
      "Requirement already satisfied: numpy in /Users/ruby/opt/anaconda3/lib/python3.8/site-packages (from torchvision) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/ruby/opt/anaconda3/lib/python3.8/site-packages (from torchvision) (4.0.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/ruby/opt/anaconda3/lib/python3.8/site-packages (from torchvision) (8.2.0)\n",
      "Requirement already satisfied: torch==1.11.0 in /Users/ruby/opt/anaconda3/lib/python3.8/site-packages (from torchvision) (1.11.0)\n",
      "Requirement already satisfied: requests in /Users/ruby/opt/anaconda3/lib/python3.8/site-packages (from torchvision) (2.25.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ruby/opt/anaconda3/lib/python3.8/site-packages (from requests->torchvision) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/ruby/opt/anaconda3/lib/python3.8/site-packages (from requests->torchvision) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ruby/opt/anaconda3/lib/python3.8/site-packages (from requests->torchvision) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/ruby/opt/anaconda3/lib/python3.8/site-packages (from requests->torchvision) (4.0.0)\n",
      "Requirement already satisfied: numpy in /Users/ruby/opt/anaconda3/lib/python3.8/site-packages (1.20.1)\n",
      "Requirement already satisfied: matplotlib in /Users/ruby/opt/anaconda3/lib/python3.8/site-packages (3.3.4)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /Users/ruby/opt/anaconda3/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/ruby/opt/anaconda3/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/ruby/opt/anaconda3/lib/python3.8/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/ruby/opt/anaconda3/lib/python3.8/site-packages (from matplotlib) (8.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/ruby/opt/anaconda3/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /Users/ruby/opt/anaconda3/lib/python3.8/site-packages (from matplotlib) (1.20.1)\n",
      "Requirement already satisfied: six in /Users/ruby/opt/anaconda3/lib/python3.8/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.5.5.64-cp36-abi3-macosx_10_15_x86_64.whl (46.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /Users/ruby/opt/anaconda3/lib/python3.8/site-packages (from opencv-python) (1.20.1)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.5.5.64\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "N_XAZJXsgyY5",
   "metadata": {
    "id": "N_XAZJXsgyY5"
   },
   "source": [
    "## import & 버전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "jEIP5rscg3ZD",
   "metadata": {
    "id": "jEIP5rscg3ZD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.8\n",
      "zsh:1: command not found: nvidia-smi\n",
      "1.11.0\n",
      "1.20.1\n",
      "0.12.0\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "!nvidia-smi\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "print(torch.__version__)\n",
    "print(np.__version__)\n",
    "print(torchvision.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4t53bdHUjmDC",
   "metadata": {
    "id": "4t53bdHUjmDC"
   },
   "source": [
    "## Hyperpatemerers 등 configs 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bduUNYwgjtE0",
   "metadata": {
    "id": "bduUNYwgjtE0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "- gpu를 사용할 수 있는 환경이면 'cuda', 그렇지 않으면 'cpu'를 사용합니다.\n",
      "lr: 0.01\n",
      "- learning rate는 이번 훈련으로 얻어낸 gradients를 얼마만큼의 강도로 모델 업데이트에 반영할지를 결정합니다.\n",
      "n_epochs: 10\n",
      "- epoch의 횟수는 전체 데이터셋을 몇 번 반복해서 훈련할 것인지를 결정합니다.\n",
      "batch_size: 512\n",
      "- 훈련 시 mini-batch의 사이즈를 결정합니다.\n",
      "- SGD(stochastic gradient descent) 기반의 optimizer를 사용한다면,\n",
      "- 전체 데이터셋을 자그마한 mini-batch로 나누어서 순차적으로 훈련에 사용하게 됩니다.\n",
      "- 보통 클수록 보다 성능을 높이는 방향으로 훈련될 가능성이 높지만 메모리를 많이 차지하게 됩니다.\n"
     ]
    }
   ],
   "source": [
    "# 훈련 결과 재현을 위한 seed값 고정\n",
    "seed = '202205281708'\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Hyperparameters\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lr = 1e-2\n",
    "n_epochs = 10\n",
    "batch_size = 512\n",
    "\n",
    "print(\"device:\", device)\n",
    "print(\"- gpu를 사용할 수 있는 환경이면 \\'cuda\\', 그렇지 않으면 \\'cpu\\'를 사용합니다.\")\n",
    "print(\"lr:\", lr)\n",
    "print(\"- learning rate는 이번 훈련으로 얻어낸 gradients를 얼마만큼의 강도로 모델 업데이트에 반영할지를 결정합니다.\")\n",
    "print(\"n_epochs:\", n_epochs)\n",
    "print(\"- epoch의 횟수는 전체 데이터셋을 몇 번 반복해서 훈련할 것인지를 결정합니다.\")\n",
    "print(\"batch_size:\", batch_size)\n",
    "print(\"- 훈련 시 mini-batch의 사이즈를 결정합니다.\")\n",
    "print(\"- SGD(stochastic gradient descent) 기반의 optimizer를 사용한다면,\")\n",
    "print(\"- 전체 데이터셋을 자그마한 mini-batch로 나누어서 순차적으로 훈련에 사용하게 됩니다.\")\n",
    "print(\"- 보통 클수록 보다 성능을 높이는 방향으로 훈련될 가능성이 높지만 메모리를 많이 차지하게 됩니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "N6HFmHqphvVO",
   "metadata": {
    "id": "N6HFmHqphvVO"
   },
   "source": [
    "## 공개 데이터셋 다운로드: MNIST\n",
    "torchvision 라이브러리를 사용하면 공개 데이터셋을 편리하게 다운로드받고, torch tensor 자료형으로 불러올 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "zmt6BsoYiiQq",
   "metadata": {
    "id": "zmt6BsoYiiQq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_data): 60000\n",
      "len(test_data): 10000\n",
      "shape: torch.Size([1, 28, 28])\n",
      "label: 3\n",
      "maxval: tensor(2.8215)\n",
      "minval: tensor(-0.4242)\n",
      "mean of all training dataset: tensor(-0.0001)\n",
      "var of all training dataset: tensor(1.0001)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# torchvision 라이브러리를 사용해 MNIST 데이터셋 다운로드 및 전처리\n",
    "transform = torchvision.transforms.Compose([\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize((0.1307,), (0.3081,)) # MNIST 데이터셋의 평균과 표준편차\n",
    "            ])\n",
    "train_data = torchvision.datasets.MNIST('./mnist_dataset', train=True, download=True,\n",
    "                            transform=transform)\n",
    "test_data = torchvision.datasets.MNIST('./mnist_dataset', train=False, download=True,\n",
    "                            transform=transform)\n",
    "print('len(train_data):', len(train_data))\n",
    "print('len(test_data):', len(test_data))\n",
    "\n",
    "# 데이터 정보를 확인해봅시다.\n",
    "sample = train_data[1234]\n",
    "print('shape:', sample[0].shape)\n",
    "print('label:', sample[1])\n",
    "print('maxval:', torch.max(sample[0]))\n",
    "print('minval:', torch.min(sample[0]))\n",
    "train_data_without_labels = torch.stack([s[0] for s in train_data])\n",
    "print('mean of all training dataset:', torch.mean(train_data_without_labels))\n",
    "print('var of all training dataset:', torch.var(train_data_without_labels))\n",
    "plt.imshow(sample[0][0])\n",
    "plt.show()\n",
    "\n",
    "# torch의 DataLoader를 사용하면 데이터셋을 준비하는 과정이 편리합니다.\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
    "                                          batch_size=1000,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "N4ffWtEayY-t",
   "metadata": {
    "id": "N4ffWtEayY-t"
   },
   "source": [
    "## 훈련 & 테스트 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36NR-n81yb0l",
   "metadata": {
    "id": "36NR-n81yb0l"
   },
   "outputs": [],
   "source": [
    "def train_model(model, device, train_loader, optimizer, n_epochs, fn_loss,\n",
    "                log_interval=10000):\n",
    "    model.train()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = fn_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % (log_interval // len(data)) == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                      epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                      100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "def test_model(model, device, test_loader, fn_loss):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdc03e1",
   "metadata": {},
   "source": [
    "## inference 위해서 모델 저장 및 불러오는 방법\n",
    "    1. 데이터 불러올 때 필요한 라이브러리들 불러오기\n",
    "    2. 신경망을 구성하고 초기화하기\n",
    "    3. 옵티마이저 초기화하기\n",
    "    4. state_dict 을 통해 모델을 저장하고 불러오기\n",
    "    5. 전체 모델을 저장하고 불러오기\n",
    "        # 경로 지정\n",
    "        PATH = \"entire_model.pt\"\n",
    "        # 저장하기\n",
    "        torch.save(net, PATH)\n",
    "        # 불러오기\n",
    "        model = torch.load(PATH)\n",
    "        model.eval()\n",
    "        \n",
    "        \n",
    "추론을 할때는 학습 단계에서 처럼 DataLoader를 활용할 필요 없음.\n",
    "- 일반적으로 추론은 이미지가 1장씩 Input으로 들어가게 되기 때문이며, 미래에 획득할 이미지를 추론한다는 의미에서 Real-Time을 가정했을때 Batch단위로 들어갈 일이 없기 때문이다. (수집된 이미지를 저장해두었다가 추론하는 경우는 DataLoader를 활용하는것이 좋음)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NFfzHcVFq0dN",
   "metadata": {
    "id": "NFfzHcVFq0dN"
   },
   "source": [
    "## 예제 1: Linear layer 1개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7jeFas2enrEe",
   "metadata": {
    "id": "7jeFas2enrEe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ModelV1...\n",
      "loaded ModelV1 to cpu\n",
      "ModelV1(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ModelV1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelV1, self).__init__()\n",
    "        self.input_size = 28*28\n",
    "        self.output_size = 10\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.input_size, self.output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    \n",
    "print(\"loading ModelV1...\") \n",
    "model = ModelV1().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "fn_loss = F.nll_loss\n",
    "print(\"loaded ModelV1 to {}\".format(device))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dca7e3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters:  7850\n",
      "device: cpu\n",
      "lr: 0.01\n",
      "n_epochs: 10\n",
      "batch_size: 512\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.316617\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.319297\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.382666\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.294995\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.295121\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.242542\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.318029\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.299951\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.283152\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.283107\n",
      "\n",
      "Test set: Average loss: 0.3216, Accuracy: 9099/10000 (91%)\n",
      "\n",
      "Train/Eval done. elapsed: 60.15913430200089 sec\n"
     ]
    }
   ],
   "source": [
    "print('Number of model parameters: ', sum(param.numel() for param in model.parameters()))\n",
    "print(\"device:\", device)\n",
    "print(\"lr:\", lr)\n",
    "print(\"n_epochs:\", n_epochs)\n",
    "print(\"batch_size:\", batch_size)\n",
    "\n",
    "t = time.perf_counter()\n",
    "train_model(model, device, train_loader, optimizer, n_epochs, fn_loss, log_interval=60000)\n",
    "test_model(model, device, test_loader, fn_loss)\n",
    "print(\"Train/Eval done. elapsed: {} sec\".format(time.perf_counter() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9d686a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def save_model(model, PATH):\n",
    "    #PATH = \"~/Documents/MLopsproject/mlops-project-study/2022-05-30/homework/model/model.pt\"\n",
    "    torch.save(model, PATH)\n",
    "    \n",
    "def load_model(PATH):\n",
    "    model = torch.load(PATH)\n",
    "    model.eval()\n",
    "    return model\n",
    "    \n",
    "def classification_img(img_path, model):\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    print(img)\n",
    "    #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # opencv는 BGR순서로 read한다.\n",
    "    img = torch.from_numpy(img).float()\n",
    "    #img = img.permute(2, 0, 1).squeeze(0) # (H, W, C) -> (C, H, W) -> (1, C, H, W)\n",
    "\n",
    "    model = model.to(device)\n",
    "    img = img.to(device)\n",
    "\n",
    "    model.eval()    #nn.Module에서 train time과 eval time에서 수행하는 다른 작업을 수행할 수 있도록 switching 하는 함수\n",
    "    criterion = torch.nn.Softmax()\n",
    "    out = model(img)\n",
    "    label_idx = torch.argmax(out, dim=1)\n",
    "    prob = criterion(out)\n",
    "\n",
    "    return prob, label_idx\n",
    "\n",
    "\n",
    "def inference_model(model,infer_img_dir):\n",
    "    for img in os.scandir(infer_img_dir):\n",
    "        img_path = os.path.join(infer_img_dir, img.name)\n",
    "\n",
    "    pred, label_idx = classification_img(img_path, model)\n",
    "\n",
    "    #label_name = label_map[label_idx]\n",
    "    print(f'Prob of {label_name}: {100 * prob:.4f}%')\n",
    "    \n",
    "    \n",
    "def inference_model_mnist_img(model, img):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    img = img.to(device)\n",
    "\n",
    "    model.eval()    #nn.Module에서 train time과 eval time에서 수행하는 다른 작업을 수행할 수 있도록 switching 하는 함수\n",
    "    #criterion = torch.nn.Softmax()\n",
    "    #prob = criterion(out)\n",
    "    out = model(img)\n",
    "    label_idx = torch.argmax(out, dim=1)\n",
    "\n",
    "    print(\"inference : \", label_idx[0])\n",
    "    #print(f'Prob of {label_name}: {100 * prob:.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fdb0674b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([1, 28, 28])\n",
      "label: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference :  tensor(3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#save_model(model, \"mnist-model.pt\")\n",
    "model = load_model(\"mnist-model.pt\")\n",
    "\n",
    "sample = train_data[30]\n",
    "print('shape:', sample[0].shape)\n",
    "print('label:', sample[1])\n",
    "\n",
    "plt.imshow(sample[0][0])\n",
    "plt.show()\n",
    "\n",
    "inference_model_mnist_img(model, sample[0])\n",
    "#inference_model(model,\"/Users/ruby/Documents/MLopsproject/mlops-project-study/2022-05-30/homework/mnist_infer\")\n",
    "\n",
    "#del model\n",
    "#del optimizer\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uezf4X4a5oHD",
   "metadata": {
    "id": "uezf4X4a5oHD"
   },
   "source": [
    "## 예제 2: Linear layer 3개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hbXnlqCr5uYm",
   "metadata": {
    "id": "hbXnlqCr5uYm"
   },
   "outputs": [],
   "source": [
    "class ModelV2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelV2, self).__init__()\n",
    "        self.input_size = 28*28\n",
    "        self.hidden_size = 256\n",
    "        self.output_size = 10\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.input_size, self.hidden_size),\n",
    "            torch.nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            torch.nn.Linear(self.hidden_size, self.output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            x = F.relu(x) if idx != len(self.layers)-1 else x\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "print(\"loading ModelV2...\") \n",
    "model = ModelV2().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "fn_loss = F.nll_loss\n",
    "print(\"loaded ModelV2 to {}\".format(device))\n",
    "print(model)\n",
    "print('Number of model parameters: ', sum(param.numel() for param in model.parameters()))\n",
    "print(\"device:\", device)\n",
    "print(\"lr:\", lr)\n",
    "print(\"n_epochs:\", n_epochs)\n",
    "print(\"batch_size:\", batch_size)\n",
    "\n",
    "t = time.perf_counter()\n",
    "train_model(model, device, train_loader, optimizer, n_epochs, fn_loss, log_interval=60000)\n",
    "test_model(model, device, test_loader, fn_loss)\n",
    "print(\"Train/Eval done. elapsed: {} sec\".format(time.perf_counter() - t))\n",
    "\n",
    "del model\n",
    "del optimizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vu9CjxFr-Fqf",
   "metadata": {
    "id": "Vu9CjxFr-Fqf"
   },
   "source": [
    "## 예제 3: CNN, linear, 각종 함수 마음껏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "qT8Vx9Wq-etV",
   "metadata": {
    "id": "qT8Vx9Wq-etV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ModelV3...\n",
      "loaded ModelV3 to cpu\n",
      "ModelV3(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (dropout1): Dropout(p=0.25, inplace=False)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "Number of model parameters:  1199882\n",
      "device: cpu\n",
      "lr: 0.01\n",
      "n_epochs: 10\n",
      "batch_size: 512\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.312362\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.716779\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.459728\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.450340\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.466849\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.281418\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.301595\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.302102\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.224618\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.338653\n",
      "\n",
      "Test set: Average loss: 0.1525, Accuracy: 9549/10000 (95%)\n",
      "\n",
      "None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got NoneType)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-33b948979b3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mnist-model3.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mnist-model3.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0minference_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"/Users/ruby/Documents/MLopsproject/mlops-project-study/2022-05-30/homework/mnist_infer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train/Eval done. elapsed: {} sec\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-1ef0160301e2>\u001b[0m in \u001b[0;36minference_model\u001b[0;34m(model, infer_img_dir)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfer_img_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mlabel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-1ef0160301e2>\u001b[0m in \u001b[0;36mclassification_img\u001b[0;34m(img_path, model)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # opencv는 BGR순서로 read한다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;31m#img = img.permute(2, 0, 1).squeeze(0) # (H, W, C) -> (C, H, W) -> (1, C, H, W)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got NoneType)"
     ]
    }
   ],
   "source": [
    "class ModelV3(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelV3, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = torch.nn.Dropout(0.25)\n",
    "        self.dropout2 = torch.nn.Dropout(0.5)\n",
    "        self.fc1 = torch.nn.Linear(9216, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "print(\"loading ModelV3...\") \n",
    "model = ModelV3().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "fn_loss = F.nll_loss\n",
    "print(\"loaded ModelV3 to {}\".format(device))\n",
    "print(model)\n",
    "print('Number of model parameters: ', sum(param.numel() for param in model.parameters()))\n",
    "print(\"device:\", device)\n",
    "print(\"lr:\", lr)\n",
    "print(\"n_epochs:\", n_epochs)\n",
    "print(\"batch_size:\", batch_size)\n",
    "\n",
    "t = time.perf_counter()\n",
    "train_model(model, device, train_loader, optimizer, n_epochs, fn_loss, log_interval=60000)\n",
    "test_model(model, device, test_loader, fn_loss)\n",
    "\n",
    "#save_model(model, \"mnist-model3.pt\")\n",
    "#model = load_model(\"mnist-model3.pt\")\n",
    "#inference_model(model,\"/Users/ruby/Documents/MLopsproject/mlops-project-study/2022-05-30/homework/mnist_infer\")\n",
    "#print(\"Train/Eval done. elapsed: {} sec\".format(time.perf_counter() - t))\n",
    "\n",
    "#del model\n",
    "#del optimizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YcfmyubEH1CO",
   "metadata": {
    "id": "YcfmyubEH1CO"
   },
   "source": [
    "## 예제 4: ModelV3의 optimizer를 Adam으로 교체, LR decaying 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_wOObXD0I9ol",
   "metadata": {
    "id": "_wOObXD0I9ol"
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def train_model_scheduler(model, device, train_loader, optimizer, n_epochs, fn_loss,\n",
    "                          scheduler, log_interval=10000):\n",
    "    model.train()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = fn_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % (log_interval // len(data)) == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                      epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                      100. * batch_idx / len(train_loader), loss.item()))\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "print(\"loading ModelV3...\") \n",
    "model = ModelV3().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "fn_loss = F.nll_loss\n",
    "\n",
    "print(\"loaded ModelV3 to {}\".format(device))\n",
    "print(model)\n",
    "print('Number of model parameters: ', sum(param.numel() for param in model.parameters()))\n",
    "print(\"device:\", device)\n",
    "print(\"lr:\", lr)\n",
    "print(\"n_epochs:\", n_epochs)\n",
    "print(\"batch_size:\", batch_size)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "\n",
    "t = time.perf_counter()\n",
    "train_model_scheduler(model, device, train_loader, optimizer, n_epochs, fn_loss, scheduler, log_interval=60000)\n",
    "test_model(model, device, test_loader, fn_loss)\n",
    "print(\"Train/Eval done. elapsed: {} sec\".format(time.perf_counter() - t))\n",
    "\n",
    "del model\n",
    "del optimizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biViwCiVs87B",
   "metadata": {
    "id": "biViwCiVs87B"
   },
   "source": [
    "## 예제 5: 훈련 완료된 빅 모델 가져와서 전이 학습(transfer learning)\n",
    "원래 ImageNet이라는 훨씬 큰 classification dataset에 맞춰서 훈련이 완료된 모델을 가져와서 MNIST 맞춤형으로 전이 학습을 시도\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auNo8-GCtfq9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "auNo8-GCtfq9",
    "outputId": "03ca19dc-24d2-4668-a7c0-d54cf2873065"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading MNISTBigModel...\n",
      "loaded MNISTBigModel to cuda\n",
      "MNISTBigModel(\n",
      "  (model): ResNet(\n",
      "    (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of model parameters:  23522250\n",
      "device: cuda\n",
      "lr: 3e-06\n",
      "n_epochs: 3\n",
      "batch_size: 64\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.468372\n",
      "Train Epoch: 1 [9984/60000 (17%)]\tLoss: 1.174733\n",
      "Train Epoch: 1 [19968/60000 (33%)]\tLoss: 0.392050\n",
      "Train Epoch: 1 [29952/60000 (50%)]\tLoss: 0.228549\n",
      "Train Epoch: 1 [39936/60000 (67%)]\tLoss: 0.134468\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.095007\n",
      "Train Epoch: 1 [59904/60000 (100%)]\tLoss: 0.112732\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.083218\n",
      "Train Epoch: 2 [9984/60000 (17%)]\tLoss: 0.064563\n",
      "Train Epoch: 2 [19968/60000 (33%)]\tLoss: 0.072744\n",
      "Train Epoch: 2 [29952/60000 (50%)]\tLoss: 0.051085\n",
      "Train Epoch: 2 [39936/60000 (67%)]\tLoss: 0.021912\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.021106\n",
      "Train Epoch: 2 [59904/60000 (100%)]\tLoss: 0.038463\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.023147\n",
      "Train Epoch: 3 [9984/60000 (17%)]\tLoss: 0.051638\n",
      "Train Epoch: 3 [19968/60000 (33%)]\tLoss: 0.030683\n",
      "Train Epoch: 3 [29952/60000 (50%)]\tLoss: 0.020306\n",
      "Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.079340\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.060572\n",
      "Train Epoch: 3 [59904/60000 (100%)]\tLoss: 0.006040\n",
      "\n",
      "Test set: Average loss: 0.0283, Accuracy: 9926/10000 (99%)\n",
      "\n",
      "Train/Eval done. elapsed: 2128.349693376 sec\n"
     ]
    }
   ],
   "source": [
    "# ref: https://ingu627.github.io/code/ResNet50_pytorch/\n",
    "class MNISTBigModel(torch.nn.Module): # MnistResNet은 nn.Module 상속\n",
    "    def __init__(self, in_channels=1):\n",
    "        super(MNISTBigModel, self).__init__()\n",
    "\n",
    "        # torchvision.models에서 사전훈련된 resnet 모델 가져오기\n",
    "        self.model = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "        # 기본 채널이 3(RGB)이기 때문에 fashion_mnist에 맞게 1(grayscale image)로 바꿔준다.  \n",
    "        # 원래 ResNet의 첫번째 층\n",
    "        # self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.model.conv1 = torch.nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    \n",
    "        # 1000개 클래스 대신 10개 클래스로 바꿔주기\n",
    "        num_ftrs = self.model.fc.in_features\n",
    "        #  nn.Linear(in_features, out_features ...)\n",
    "        self.model.fc = torch.nn.Linear(num_ftrs, 10)\n",
    "        #num_ftrs = self.model.fc.out_features\n",
    "        #self.projection_layer = torch.nn.Linear(num_ftrs, 10)\n",
    "\n",
    "    def forward(self, x): # 모델에 있는 foward 함수 그대로 가져오기\n",
    "        #with torch.no_grad():\n",
    "        x = self.model(x)\n",
    "        #x = self.projection_layer(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "lr = 3e-6\n",
    "n_epochs = 3\n",
    "batch_size = 64\n",
    "\n",
    "# MNIST 그림의 크기를 ImageNet 데이터와 맞추기 위해 데이터셋을 다시 로드\n",
    "transform = torchvision.transforms.Compose([\n",
    "                torchvision.transforms.Resize((224, 224)),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize((0.1307,), (0.3081,)) # MNIST 데이터셋의 평균과 표준편차\n",
    "            ])\n",
    "train_data = torchvision.datasets.MNIST('./mnist_dataset', train=True, download=True,\n",
    "                            transform=transform)\n",
    "test_data = torchvision.datasets.MNIST('./mnist_dataset', train=False, download=True,\n",
    "                            transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
    "                                          batch_size=10,\n",
    "                                          shuffle=True)\n",
    "\n",
    "print(\"loading MNISTBigModel...\") \n",
    "model = MNISTBigModel().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "fn_loss = F.nll_loss\n",
    "\n",
    "print(\"loaded MNISTBigModel to {}\".format(device))\n",
    "print(model)\n",
    "print('Number of model parameters: ', sum(param.numel() for param in model.parameters()))\n",
    "print(\"device:\", device)\n",
    "print(\"lr:\", lr)\n",
    "print(\"n_epochs:\", n_epochs)\n",
    "print(\"batch_size:\", batch_size)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "\n",
    "t = time.perf_counter()\n",
    "train_model_scheduler(model, device, train_loader, optimizer, n_epochs, fn_loss, scheduler, log_interval=10000)\n",
    "test_model(model, device, test_loader, fn_loss)\n",
    "print(\"Train/Eval done. elapsed: {} sec\".format(time.perf_counter() - t))\n",
    "\n",
    "del model\n",
    "del optimizer\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "training_remind_seminar.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
